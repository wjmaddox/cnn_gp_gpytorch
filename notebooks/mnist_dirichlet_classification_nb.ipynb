{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "\n",
    "import cnn_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from kernels.cnn_kernel import CNNGP_Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load dataset\n",
    "mnist = torchvision.datasets.MNIST('/home/wesley/Documents/datasets/', download=True, \n",
    "                                   transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### construct training and test datasets (batched for now)\n",
    "\n",
    "iter_data_loader = iter(torch.utils.data.DataLoader(mnist, batch_size = 128))\n",
    "\n",
    "inputs, targets = next(iter_data_loader)\n",
    "targets = targets.float()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    \n",
    "test_inputs, test_targets = next(iter_data_loader)\n",
    "test_targets = test_targets.float()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    test_inputs, test_targets = test_inputs.cuda(), test_targets.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define model from resnet on mnist\n",
    "in_channels = 1\n",
    "out_channels = 10\n",
    "\n",
    "var_bias = 0.86\n",
    "var_weight = 0.79\n",
    "\n",
    "layers = []\n",
    "for _ in range(7):  # n_layers\n",
    "    layers += [\n",
    "        cnn_gp.Conv2d(kernel_size=7, padding=\"same\", var_weight=var_weight,\n",
    "               var_bias=var_bias),\n",
    "        cnn_gp.ReLU(),\n",
    "    ]\n",
    "initial_model = cnn_gp.Sequential(\n",
    "    *layers,\n",
    "    cnn_gp.Conv2d(kernel_size=28, padding=0, var_weight=var_weight,\n",
    "           var_bias=var_bias),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPClassificationModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, cnn_model, input_shape, train_x, train_y, alpha_epsilon = 0.01):\n",
    "        num_classes = train_y.max() + 1\n",
    "        \n",
    "        # set alpha = \\alpha_\\epsilon\n",
    "        alpha = alpha_epsilon * torch.ones(train_x.shape[-2], num_classes, \n",
    "                                           device = train_x.device, dtype = train_x.dtype)\n",
    "        \n",
    "        # alpha[class_labels] = 1 + \\alpha_\\epsilon\n",
    "        alpha[torch.arange(len(train_x)), train_y] = alpha[torch.arange(len(train_x)), train_y] + 1.\n",
    "        \n",
    "        # sigma^2 = log(1 / alpha + 1)\n",
    "        sigma2_i = torch.log(1 / alpha + 1.)\n",
    "        \n",
    "        # y = log(alpha) - 0.5 * sigma^2\n",
    "        transformed_targets = alpha.log() - 0.5 * sigma2_i\n",
    "        \n",
    "        likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(sigma2_i.t(), learn_additional_noise= True)\n",
    "        \n",
    "        super(DPClassificationModel, self).__init__(train_x, transformed_targets.t(), likelihood)\n",
    "        self.transformed_targets = transformed_targets\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_size = num_classes)\n",
    "        self.covar_module = CNNGP_Kernel(cnn_model, input_shape, batch_shape=torch.Size((num_classes,)))\n",
    "        \n",
    "        self.likelihood = likelihood\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### define model\n",
    "### TODO: check that our 2d reshaping is reasonable\n",
    "\n",
    "#ikelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = DPClassificationModel(initial_model, inputs.shape[1:], train_x = inputs.view(inputs.shape[0], 28*28), \n",
    "                              train_y=targets.long())\n",
    "likelihood = model.likelihood\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    likelihood = likelihood.cuda()\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10]) False\n",
      "shape of kernel is:  torch.Size([128, 128])\n",
      "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor object at 0x7f89709eef10> torch.Size([128, 128])\n",
      "inside:  torch.Size([1, 128, 128])\n",
      "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor object at 0x7f88f6c0a850> torch.Size([1, 128, 128])\n",
      "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor object at 0x7f88f6c0a850> torch.Size([1, 128, 128])\n",
      "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor object at 0x7f88f6c0a850> torch.Size([1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley/Documents/Code/gpytorch/gpytorch/utils/cholesky.py:42: RuntimeWarning: A not p.d., added jitter of 9.999999999999999e-06 to the diagonal\n",
      "  warnings.warn(f\"A not p.d., added jitter of {jitter_new} to the diagonal\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-52ea2ffd01fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Code/gpytorch/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mrsample\u001b[0;34m(self, sample_shape, base_samples)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# Get samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_mean_mvn_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36mzero_mean_mvn_samples\u001b[0;34m(self, num_samples)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovar_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m         )\n\u001b[0;32m-> 1593\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovar_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/wesley/Documents/Code/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m(1593)\u001b[0;36mzero_mean_mvn_samples\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1591 \u001b[0;31m            \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovar_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1592 \u001b[0;31m        )\n",
      "\u001b[0m\u001b[0;32m-> 1593 \u001b[0;31m        \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovar_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1594 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1595 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "model(inputs.view(inputs.shape[0], 28*28)).rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim.fit import fit_gpytorch_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define and fit any free parameters in the model\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "fit_gpytorch_torch(mll, options={\"maxiter\": 1000, \"disp\": True, \"lr\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set to test mode\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model(test_inputs.view(test_inputs.shape[0], 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### yikes :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor object at 0x7f88f00fc590> torch.Size([30, 30])\n",
      "inside:  torch.Size([1, 30, 30])\n",
      "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor object at 0x7f88f04ea5d0> torch.Size([1, 30, 30])\n",
      "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor object at 0x7f88f010df90> torch.Size([1, 30, 30])\n",
      "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor object at 0x7f88f010df90> torch.Size([1, 30, 30])\n",
      "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor object at 0x7f88f010df90> torch.Size([1, 30, 30])\n",
      "<gpytorch.lazy.batch_repeat_lazy_tensor.BatchRepeatLazyTensor object at 0x7f88f00fc750> torch.Size([10, 30, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.8400,   0.3161,  -0.7144,  -0.3811,  -2.6392,   9.1203, -15.6471,\n",
       "           6.8704,   3.8084,  -1.1872, -11.0476,  -2.6245,  -8.4925,  16.3853,\n",
       "          -4.7919,  -5.1416,   0.8967,  -0.2628,  14.0446,   5.1566,  -6.2106,\n",
       "         -11.7443,  -7.3664,   7.7204,  -0.7164, -11.3812,   3.0444,   6.4354,\n",
       "           0.0371, -11.3726],\n",
       "        [ -2.4838,   7.8087,  -5.7541,   4.9832,  -4.8660,   7.4775, -13.1246,\n",
       "           5.0781,   1.5952,  -2.6066,   5.6760,   5.3088,   6.1152,  -2.1733,\n",
       "          -2.0447,   2.7258,  12.3781,  -0.5361,  -4.8066,   0.3408,  -0.1203,\n",
       "           2.9489,  -8.4204,  13.0459,   7.4378,   6.4842,  -2.9404,  -2.6798,\n",
       "           3.5718,   4.4365],\n",
       "        [  5.9035,   3.5853,  -6.6984,   8.7704,   2.8835,   5.8287,  -7.8802,\n",
       "           4.2365,  -6.2981,  -1.8538,   9.5167,  -4.6293,   7.8201,   3.0214,\n",
       "          -8.4342,  -0.6910,   1.7082,   7.7967, -10.2497,   3.3097,   2.9738,\n",
       "          -7.1227,  -0.7410,   4.4709,   3.2216,  -0.2859,  -3.2544,  -1.2527,\n",
       "           1.5485,   9.9106],\n",
       "        [  3.4474,   1.7526,  -2.4786,  -2.1940,  -0.1657,   1.4246,   8.0279,\n",
       "          -1.2436, -13.0258,  -5.7650,   1.6803,   1.0307,   0.6788,  -6.0528,\n",
       "          -5.8463,   0.5287,   0.0237,   6.8418,  -5.5233,  -7.6423,  -3.3978,\n",
       "           5.8151,   7.1051,   4.3485,  -3.5217,   9.0024,  -0.0599,  -3.7778,\n",
       "          -3.4511,  12.7317],\n",
       "        [  3.6660,   0.3246,  -1.0289,   3.6436,   2.5227,   0.0842,   0.4391,\n",
       "           4.6657,   4.6966,  -5.0048,  -8.1283,   2.8528,  -5.7630,  -2.6156,\n",
       "          -1.7713,  -2.4932,   1.7766,  11.5209,  -4.0371,  -0.3418,   0.9125,\n",
       "           0.0386,  13.6465,  -6.7187,  -7.0924,   3.9464,  -0.5672,   6.3917,\n",
       "          -1.3252,   9.6507],\n",
       "        [-10.8612,  -1.4502,   0.8315,  -2.1607, -11.9418,   3.2170,  -9.7294,\n",
       "           3.4407,  -3.9656, -14.4990, -12.9505,  -2.7209,  -5.7478,   1.7265,\n",
       "           9.8728,   2.8662, -10.2257,   1.3698,  -0.1224,   2.7620,  -0.4526,\n",
       "           3.9214,  -1.8325,   6.5176, -11.6245,  -9.6646,   1.9588,  -6.6730,\n",
       "           6.0396,  -4.8382],\n",
       "        [ 12.3622,  -4.4988,   3.8029,   1.3594,  -0.4576,  -3.3321,   6.4424,\n",
       "         -10.0450,  -6.4508,  -7.2576,  -2.7105,   1.8217,  11.3928, -11.2679,\n",
       "           5.9265,  -7.4454,  -5.0720,   0.9906,   7.7252,   8.2621,  12.5059,\n",
       "          -6.5503,  -3.4555,  -2.7668,  -9.0533,   0.5576,  -0.1216,   3.3025,\n",
       "          -8.3778,   0.3473],\n",
       "        [ -1.8753,   2.3237,  -6.6511,   3.1083,   6.3340,   4.0964,   4.9311,\n",
       "          -3.2938,   5.3965,   6.3471,  -3.0972,  -4.6934,  -0.2233,  -0.5658,\n",
       "           5.3043,   5.2027,   4.6335,   0.1931,  -0.9058,   2.2777,  -5.0854,\n",
       "           4.9229,   2.1082,  -7.1351,  -1.5763,  -4.1626,  -0.1585,  -2.0284,\n",
       "         -13.5357,  -3.6673],\n",
       "        [ -1.3426,   4.2804,  -3.5153,  -1.7078,  -2.6407,   4.6026, -17.9203,\n",
       "           4.9007,  -0.0466,  -1.0893,  -2.8232,   8.0186,   4.1861,   4.9042,\n",
       "          -3.3073,  -4.5201,  -4.3751,   5.2049,  -3.8517,   5.7270,  -2.7743,\n",
       "          -6.7494, -14.9023,   0.3500,   5.6597,  -1.8074,  -1.3363,  -5.1522,\n",
       "           5.6877,  -1.6125],\n",
       "        [  1.0110,  12.4246,   2.2932,  -0.7117,   0.6870,   8.5043,  -2.7295,\n",
       "           5.4445, -15.3997,  -4.0018,   0.6750,   0.4614,   4.0301,   6.6945,\n",
       "          -1.1118,   4.5507,   1.3701,  -2.1737, -11.2644,  -2.8138,  -9.3861,\n",
       "           0.2429, -10.2209,  11.3501,   0.7986,  -1.6132,   2.9957, -19.1797,\n",
       "           7.6815,   2.1575]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(513, 513)\n",
    "symm_a = a.t() @ a\n",
    "covar = gpytorch.lazy.BatchRepeatLazyTensor(gpytorch.lazify(symm_a), torch.Size((10,)))\n",
    "mean = torch.randn(10, 30)\n",
    "\n",
    "dist = gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "dist.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
